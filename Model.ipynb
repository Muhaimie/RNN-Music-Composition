{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "music21: Certain music21 functions might need the optional package scipy;\n",
      "                  if you run into errors, install it by following the instructions at\n",
      "                  http://mit.edu/music21/doc/installing/installAdditional.html\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import glob\n",
    "import random\n",
    "import pretty_midi\n",
    "import music21\n",
    "import IPython \n",
    "import numpy as np\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "from random import shuffle, seed\n",
    "import numpy as np\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change the midi song key to C major or A minor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#major conversion\n",
    "majors = dict([(\"A-\",4),(\"G#\",4),(\"A\",3),(\"A#\",2),(\"B-\",2),(\"B\",1),(\"C\",0),(\"C#\",-1),(\"D-\",-1),(\"D\",-2),(\"D#\",-3),(\"E-\",-3),(\"E\",-4),(\"F\",-5),(\"F#\",6),(\"G-\",6),(\"G\",5)])\n",
    "minors = dict([(\"G#\",1),(\"A-\",1),(\"A\",0),(\"A#\",-1),(\"B-\",-1),(\"B\",-2),(\"C\",-3),(\"C#\",-4),(\"D-\",-4),(\"D\",-5),(\"D#\",6),(\"E-\",6),(\"E\",5),(\"F\",4),(\"F#\",3),(\"G-\",3),(\"G\",2)])\n",
    "\n",
    "for file in glob.glob(\"chopin/*.mid\"):\n",
    "    score = music21.converter.parse(file)\n",
    "    key = score.analyze('key')\n",
    "    \n",
    "    \n",
    "    if key.mode == \"major\":\n",
    "        halfSteps = majors[key.tonic.name]\n",
    "        \n",
    "    elif key.mode == \"minor\":\n",
    "        halfSteps = minors[key.tonic.name]\n",
    "        \n",
    "    newscore = score.transpose(halfSteps)\n",
    "    key = newscore.analyze('key')\n",
    "    \n",
    "    newFileName = \"C_\" + file\n",
    "    newscore.write('midi',newFileName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GET THE PIANO ROLL USING THE PRETTY MIDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piano_roll_to_pretty_midi(piano_roll, fs=100, program=0):\n",
    "    '''Convert a Piano Roll array into a PrettyMidi object\n",
    "     with a single instrument.\n",
    "    Parameters\n",
    "    ----------\n",
    "    piano_roll : np.ndarray, shape=(128,frames), dtype=int\n",
    "        Piano roll of one instrument\n",
    "    fs : int\n",
    "        Sampling frequency of the columns, i.e. each column is spaced apart\n",
    "        by ``1./fs`` seconds.\n",
    "    program : int\n",
    "        The program number of the instrument.\n",
    "    Returns\n",
    "    -------\n",
    "    midi_object : pretty_midi.PrettyMIDI\n",
    "        A pretty_midi.PrettyMIDI class instance describing\n",
    "        the piano roll.\n",
    "    '''\n",
    "    notes, frames = piano_roll.shape\n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "    instrument = pretty_midi.Instrument(program=program)\n",
    "\n",
    "    # pad 1 column of zeros so we can acknowledge inital and ending events\n",
    "    piano_roll = np.pad(piano_roll, [(0, 0), (1, 1)], 'constant')\n",
    "\n",
    "    # use changes in velocities to find note on / note off events\n",
    "    velocity_changes = np.nonzero(np.diff(piano_roll).T)\n",
    "\n",
    "    # keep track on velocities and note on times\n",
    "    prev_velocities = np.zeros(notes, dtype=int)\n",
    "    note_on_time = np.zeros(notes)\n",
    "\n",
    "    for time, note in zip(*velocity_changes):\n",
    "        # use time + 1 because of padding above\n",
    "        velocity = piano_roll[note, time + 1]\n",
    "        time = time / fs\n",
    "        if velocity > 0:\n",
    "            if prev_velocities[note] == 0:\n",
    "                note_on_time[note] = time\n",
    "                prev_velocities[note] = velocity\n",
    "        else:\n",
    "            pm_note = pretty_midi.Note(\n",
    "                velocity=prev_velocities[note],\n",
    "                pitch=note,\n",
    "                start=note_on_time[note],\n",
    "                end=time)\n",
    "            instrument.notes.append(pm_note)\n",
    "            prev_velocities[note] = 0\n",
    "    pm.instruments.append(instrument)\n",
    "    return pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GET ALL THE MIDI FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_list_midi(folder = 'c_chopin/*.mid', seed_int = 666):\n",
    "  \"\"\"Get the list of all midi file in the folders\n",
    "  \n",
    "  Parameters\n",
    "  ==========\n",
    "  folder : str\n",
    "    The midi folder.\n",
    "  seed_int : int\n",
    "    the random seed.\n",
    "  \n",
    "  Returns\n",
    "  =======\n",
    "  The midi files\n",
    "  \n",
    "  \"\"\"\n",
    "  list_all_midi = glob.glob(folder)\n",
    "  seed(seed_int)\n",
    "  shuffle(list_all_midi)\n",
    "  return list_all_midi\n",
    "\n",
    "list_all_midi = get_list_midi()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoteTokenizer:\n",
    "    \n",
    "    def __init__(self):\n",
    "      self.notes_to_index = {}\n",
    "      self.index_to_notes = {}\n",
    "      self.num_of_word = 0\n",
    "      self.unique_word = 0\n",
    "      self.notes_freq = {}\n",
    "        \n",
    "    def transform(self,list_array):\n",
    "      \"\"\" Transform a list of note in string into index.\n",
    "      \n",
    "      Parameters\n",
    "      ==========\n",
    "      list_array : list\n",
    "        list of note in string format\n",
    "      \n",
    "      Returns\n",
    "      =======\n",
    "      The transformed list in numpy array.\n",
    "      \n",
    "      \"\"\"\n",
    "      transformed_list = []\n",
    "      for instance in list_array:\n",
    "          transformed_list.append([self.notes_to_index[note] for note in instance])\n",
    "      return np.array(transformed_list, dtype=np.int32)\n",
    " \n",
    "    def partial_fit(self, notes):\n",
    "        \"\"\" Partial fit on the dictionary of the tokenizer\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        notes : list of notes\n",
    "        \n",
    "        \"\"\"\n",
    "        for note in notes:\n",
    "            note_str = ','.join(str(a) for a in note)\n",
    "            if note_str in self.notes_freq:\n",
    "                self.notes_freq[note_str] += 1\n",
    "                self.num_of_word += 1\n",
    "            else:\n",
    "                self.notes_freq[note_str] = 1\n",
    "                self.unique_word += 1\n",
    "                self.num_of_word += 1\n",
    "                self.notes_to_index[note_str], self.index_to_notes[self.unique_word] = self.unique_word, note_str\n",
    "            \n",
    "    def add_new_note(self, note):\n",
    "        \"\"\" Add a new note into the dictionary\n",
    "\n",
    "        Parameters\n",
    "        ==========\n",
    "        note : str\n",
    "          a new note who is not in dictionary.  \n",
    "\n",
    "        \"\"\"\n",
    "        assert note not in self.notes_to_index\n",
    "        self.unique_word += 1\n",
    "        self.notes_to_index[note], self.index_to_notes[self.unique_word] = self.unique_word, note\n",
    "        \n",
    "def generate_batch_song(list_all_midi, batch_music=16, start_index=0, fs=30, seq_len=50, use_tqdm=False):\n",
    "    \"\"\"\n",
    "    Generate Batch music that will be used to be input and output of the neural network\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    list_all_midi : list\n",
    "      List of midi files\n",
    "    batch_music : int\n",
    "      A number of music in one batch\n",
    "    start_index : int\n",
    "      The start index to be batched in list_all_midi\n",
    "    fs : int\n",
    "      Sampling frequency of the columns, i.e. each column is spaced apart\n",
    "        by ``1./fs`` seconds.\n",
    "    seq_len : int\n",
    "      The sequence length of the music to be input of neural network\n",
    "    use_tqdm : bool\n",
    "      Whether to use tqdm or not in the function\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    Tuple of input and target neural network\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(list_all_midi) >= batch_music\n",
    "    dict_time_notes = generate_dict_time_notes(list_all_midi, batch_music, start_index, fs, use_tqdm=use_tqdm)\n",
    "    \n",
    "    list_musics = process_notes_in_song(dict_time_notes, seq_len)\n",
    "    collected_list_input, collected_list_target = [], []\n",
    "     \n",
    "    for music in list_musics:\n",
    "        list_training, list_target = generate_input_and_target(music, seq_len)\n",
    "        collected_list_input += list_training\n",
    "        collected_list_target += list_target\n",
    "    return collected_list_input, collected_list_target\n",
    "\n",
    "def generate_dict_time_notes(list_all_midi, batch_song = 16, start_index=0, fs=30, use_tqdm=True):\n",
    "    \"\"\" Generate map (dictionary) of music ( in index ) to piano_roll (in np.array)\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    list_all_midi : list\n",
    "        List of midi files\n",
    "    batch_music : int\n",
    "      A number of music in one batch\n",
    "    start_index : int\n",
    "      The start index to be batched in list_all_midi\n",
    "    fs : int\n",
    "      Sampling frequency of the columns, i.e. each column is spaced apart\n",
    "        by ``1./fs`` seconds.\n",
    "    use_tqdm : bool\n",
    "      Whether to use tqdm or not in the function\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "    dictionary of music to piano_roll (in np.array)\n",
    "\n",
    "    \"\"\"\n",
    "    assert len(list_all_midi) >= batch_song\n",
    "    \n",
    "    dict_time_notes = {}\n",
    "    process_tqdm_midi = tqdm_notebook(range(start_index, min(start_index + batch_song, len(list_all_midi)))) if use_tqdm else range(start_index,  min(start_index + batch_song, len(list_all_midi)))\n",
    "    for i in process_tqdm_midi:\n",
    "        midi_file_name = list_all_midi[i]\n",
    "        if use_tqdm:\n",
    "            process_tqdm_midi.set_description(\"Processing {}\".format(midi_file_name))\n",
    "        try: # Handle exception on malformat MIDI files\n",
    "            midi_pretty_format = pretty_midi.PrettyMIDI(midi_file_name)\n",
    "            piano_midi = midi_pretty_format.instruments[0] # Get the piano channels\n",
    "            piano_roll = piano_midi.get_piano_roll(fs=fs)\n",
    "            dict_time_notes[i] = piano_roll\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"broken file : {}\".format(midi_file_name))\n",
    "            pass\n",
    "    return dict_time_notes\n",
    "\n",
    "def generate_input_and_target(dict_keys_time, seq_len=50):\n",
    "    \"\"\" Generate input and the target of our deep learning for one music.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    dict_keys_time : dict\n",
    "      Dictionary of timestep and notes\n",
    "    seq_len : int\n",
    "      The length of the sequence\n",
    "      \n",
    "    Returns\n",
    "    =======\n",
    "    Tuple of list of input and list of target of neural network.\n",
    "    \n",
    "       \n",
    "    \"\"\"\n",
    "    # Get the start time and end time\n",
    "    start_time, end_time = list(dict_keys_time.keys())[0], list(dict_keys_time.keys())[-1]\n",
    "    list_training, list_target = [], []\n",
    "    for index_enum, time in enumerate(range(start_time, end_time)):\n",
    "        list_append_training, list_append_target = [], []\n",
    "        start_iterate = 0\n",
    "        flag_target_append = False # flag to append the test list\n",
    "        if index_enum < seq_len:\n",
    "            start_iterate = seq_len - index_enum - 1\n",
    "            for i in range(start_iterate): # add 'e' to the seq list. \n",
    "                list_append_training.append('e')\n",
    "                flag_target_append = True\n",
    "\n",
    "        for i in range(start_iterate,seq_len):\n",
    "            index_enum = time - (seq_len - i - 1)\n",
    "            if index_enum in dict_keys_time:\n",
    "                list_append_training.append(','.join(str(x) for x in dict_keys_time[index_enum]))      \n",
    "            else:\n",
    "                list_append_training.append('e')\n",
    "\n",
    "        # add time + 1 to the list_append_target\n",
    "        if time+1 in dict_keys_time:\n",
    "            list_append_target.append(','.join(str(x) for x in dict_keys_time[time+1]))\n",
    "        else:\n",
    "            list_append_target.append('e')\n",
    "        list_training.append(list_append_training)\n",
    "        list_target.append(list_append_target)\n",
    "    return list_training, list_target\n",
    "\n",
    "def process_notes_in_song(dict_time_notes, seq_len = 50):\n",
    "    \"\"\"\n",
    "    Iterate the dict of piano rolls into dictionary of timesteps and note played\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    dict_time_notes : dict\n",
    "      dict contains index of music ( in index ) to piano_roll (in np.array)\n",
    "    seq_len : int\n",
    "      Length of the sequence\n",
    "      \n",
    "    Returns\n",
    "    =======\n",
    "    Dict of timesteps and note played\n",
    "    \"\"\"\n",
    "    list_of_dict_keys_time = []\n",
    "    \n",
    "    for key in dict_time_notes:\n",
    "        sample = dict_time_notes[key]\n",
    "        times = np.unique(np.where(sample > 0)[1])\n",
    "        index = np.where(sample > 0)\n",
    "        dict_keys_time = {}\n",
    "\n",
    "        for time in times:\n",
    "            index_where = np.where(index[1] == time)\n",
    "            notes = index[0][index_where]\n",
    "            dict_keys_time[time] = notes\n",
    "        list_of_dict_keys_time.append(dict_keys_time)\n",
    "    return list_of_dict_keys_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### sampled midi\n",
    "len(list_all_midi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_midi = list_all_midi[0:100]\n",
    "#actually only 52 song available "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We create a map of note -> index here using NoteTokenizer that we've defined before.\n",
    "### This object will be used to transform the list of notes to be ready for the input of Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f82a267433408d996683cfd41de288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=52.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch = 1\n",
    "start_index = 0\n",
    "note_tokenizer = NoteTokenizer()\n",
    "\n",
    "for i in tqdm_notebook(range(len(sampled_midi))):\n",
    "    dict_time_notes = generate_dict_time_notes(sampled_midi, batch_song=1, start_index=i, use_tqdm=False, fs=5)\n",
    "    full_notes = process_notes_in_song(dict_time_notes)\n",
    "    for note in full_notes:\n",
    "        note_tokenizer.partial_fit(list(note.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### if thre is no note at the timestep add 'e' instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "note_tokenizer.add_new_note('e') #add empty notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12259\n"
     ]
    }
   ],
   "source": [
    "unique_notes = note_tokenizer.unique_word\n",
    "print(unique_notes)\n",
    "#print(len(full_notes[0]))  # 2603 input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## architecture creation\n",
    "# 1.Embedding\n",
    "# 2.GRU\n",
    "# 3.Self Head Attention\n",
    "# 4.GRU\n",
    "# 5.Self Head Attention\n",
    "# 6.Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 50\n",
    "EPOCHS = 4\n",
    "BATCH_SONG = 10\n",
    "BATCH_NNET_SIZE = 96\n",
    "TOTAL_SONG = len(sampled_midi)\n",
    "FRAME_PER_SECOND = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### self attention layer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqSelfAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    ATTENTION_TYPE_ADD = 'additive'\n",
    "    ATTENTION_TYPE_MUL = 'multiplicative'\n",
    "\n",
    "    def __init__(self,\n",
    "                 units=32,\n",
    "                 attention_width=None,\n",
    "                 attention_type=ATTENTION_TYPE_ADD,\n",
    "                 return_attention=False,\n",
    "                 history_only=False,\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 use_additive_bias=True,\n",
    "                 use_attention_bias=True,\n",
    "                 attention_activation=None,\n",
    "                 attention_regularizer_weight=0.0,\n",
    "                 **kwargs):\n",
    "        \"\"\"Layer initialization.\n",
    "        For additive attention, see: https://arxiv.org/pdf/1806.01264.pdf\n",
    "        :param units: The dimension of the vectors that used to calculate the attention weights.\n",
    "        :param attention_width: The width of local attention.\n",
    "        :param attention_type: 'additive' or 'multiplicative'.\n",
    "        :param return_attention: Whether to return the attention weights for visualization.\n",
    "        :param history_only: Only use historical pieces of data.\n",
    "        :param kernel_initializer: The initializer for weight matrices.\n",
    "        :param bias_initializer: The initializer for biases.\n",
    "        :param kernel_regularizer: The regularization for weight matrices.\n",
    "        :param bias_regularizer: The regularization for biases.\n",
    "        :param kernel_constraint: The constraint for weight matrices.\n",
    "        :param bias_constraint: The constraint for biases.\n",
    "        :param use_additive_bias: Whether to use bias while calculating the relevance of inputs features\n",
    "                                  in additive mode.\n",
    "        :param use_attention_bias: Whether to use bias while calculating the weights of attention.\n",
    "        :param attention_activation: The activation used for calculating the weights of attention.\n",
    "        :param attention_regularizer_weight: The weights of attention regularizer.\n",
    "        :param kwargs: Parameters for parent class.\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.units = units\n",
    "        self.attention_width = attention_width\n",
    "        self.attention_type = attention_type\n",
    "        self.return_attention = return_attention\n",
    "        self.history_only = history_only\n",
    "        if history_only and attention_width is None:\n",
    "            self.attention_width = int(1e9)\n",
    "\n",
    "        self.use_additive_bias = use_additive_bias\n",
    "        self.use_attention_bias = use_attention_bias\n",
    "        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = tf.keras.regularizers.get(bias_regularizer)\n",
    "        self.kernel_constraint = tf.keras.constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = tf.keras.constraints.get(bias_constraint)\n",
    "        self.attention_activation = tf.keras.activations.get(attention_activation)\n",
    "        self.attention_regularizer_weight = attention_regularizer_weight\n",
    "        self._backend = tf.keras.backend.backend()\n",
    "\n",
    "        if attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n",
    "            self.Wx, self.Wt, self.bh = None, None, None\n",
    "            self.Wa, self.ba = None, None\n",
    "        elif attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n",
    "            self.Wa, self.ba = None, None\n",
    "        else:\n",
    "            raise NotImplementedError('No implementation for attention type : ' + attention_type)\n",
    "\n",
    "        super(SeqSelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units': self.units,\n",
    "            'attention_width': self.attention_width,\n",
    "            'attention_type': self.attention_type,\n",
    "            'return_attention': self.return_attention,\n",
    "            'history_only': self.history_only,\n",
    "            'use_additive_bias': self.use_additive_bias,\n",
    "            'use_attention_bias': self.use_attention_bias,\n",
    "            'kernel_initializer': tf.keras.regularizers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer': tf.keras.regularizers.serialize(self.bias_initializer),\n",
    "            'kernel_regularizer': tf.keras.regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': tf.keras.regularizers.serialize(self.bias_regularizer),\n",
    "            'kernel_constraint': tf.keras.constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': tf.keras.constraints.serialize(self.bias_constraint),\n",
    "            'attention_activation': tf.keras.activations.serialize(self.attention_activation),\n",
    "            'attention_regularizer_weight': self.attention_regularizer_weight,\n",
    "        }\n",
    "        base_config = super(SeqSelfAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if isinstance(input_shape, list):\n",
    "            input_shape = input_shape[0]\n",
    "        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n",
    "            self._build_additive_attention(input_shape)\n",
    "        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n",
    "            self._build_multiplicative_attention(input_shape)\n",
    "        super(SeqSelfAttention, self).build(input_shape)\n",
    "\n",
    "    def _build_additive_attention(self, input_shape):\n",
    "        feature_dim = input_shape[2]\n",
    "\n",
    "        self.Wt = self.add_weight(shape=(feature_dim, self.units),\n",
    "                                  name='{}_Add_Wt'.format(self.name),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  regularizer=self.kernel_regularizer,\n",
    "                                  constraint=self.kernel_constraint)\n",
    "        self.Wx = self.add_weight(shape=(feature_dim, self.units),\n",
    "                                  name='{}_Add_Wx'.format(self.name),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  regularizer=self.kernel_regularizer,\n",
    "                                  constraint=self.kernel_constraint)\n",
    "        if self.use_additive_bias:\n",
    "            self.bh = self.add_weight(shape=(self.units,),\n",
    "                                      name='{}_Add_bh'.format(self.name),\n",
    "                                      initializer=self.bias_initializer,\n",
    "                                      regularizer=self.bias_regularizer,\n",
    "                                      constraint=self.bias_constraint)\n",
    "\n",
    "        self.Wa = self.add_weight(shape=(self.units, 1),\n",
    "                                  name='{}_Add_Wa'.format(self.name),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  regularizer=self.kernel_regularizer,\n",
    "                                  constraint=self.kernel_constraint)\n",
    "        if self.use_attention_bias:\n",
    "            self.ba = self.add_weight(shape=(1,),\n",
    "                                      name='{}_Add_ba'.format(self.name),\n",
    "                                      initializer=self.bias_initializer,\n",
    "                                      regularizer=self.bias_regularizer,\n",
    "                                      constraint=self.bias_constraint)\n",
    "\n",
    "    def _build_multiplicative_attention(self, input_shape):\n",
    "        feature_dim = input_shape[2]\n",
    "\n",
    "        self.Wa = self.add_weight(shape=(feature_dim, feature_dim),\n",
    "                                  name='{}_Mul_Wa'.format(self.name),\n",
    "                                  initializer=self.kernel_initializer,\n",
    "                                  regularizer=self.kernel_regularizer,\n",
    "                                  constraint=self.kernel_constraint)\n",
    "        if self.use_attention_bias:\n",
    "            self.ba = self.add_weight(shape=(1,),\n",
    "                                      name='{}_Mul_ba'.format(self.name),\n",
    "                                      initializer=self.bias_initializer,\n",
    "                                      regularizer=self.bias_regularizer,\n",
    "                                      constraint=self.bias_constraint)\n",
    "\n",
    "    def call(self, inputs, mask=None, **kwargs):\n",
    "        if isinstance(inputs, list):\n",
    "            inputs, positions = inputs\n",
    "            positions = K.cast(positions, 'int32')\n",
    "            mask = mask[1]\n",
    "        else:\n",
    "            positions = None\n",
    "\n",
    "        input_len = K.shape(inputs)[1]\n",
    "\n",
    "        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n",
    "            e = self._call_additive_emission(inputs)\n",
    "        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n",
    "            e = self._call_multiplicative_emission(inputs)\n",
    "\n",
    "        if self.attention_activation is not None:\n",
    "            e = self.attention_activation(e)\n",
    "        e = K.exp(e - K.max(e, axis=-1, keepdims=True))\n",
    "        if self.attention_width is not None:\n",
    "            ones = tf.ones((input_len, input_len))\n",
    "            if self.history_only:\n",
    "                local = tf.linalg.band_part(\n",
    "                    ones,\n",
    "                    K.minimum(input_len, self.attention_width - 1),\n",
    "                    0,\n",
    "                )\n",
    "            else:\n",
    "                local = tf.linalg.band_part(\n",
    "                    ones,\n",
    "                    K.minimum(input_len, self.attention_width // 2),\n",
    "                    K.minimum(input_len, (self.attention_width - 1) // 2),\n",
    "                )\n",
    "            e = e * K.expand_dims(local, 0)\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            mask = K.expand_dims(mask)\n",
    "            e = K.permute_dimensions(K.permute_dimensions(e * mask, (0, 2, 1)) * mask, (0, 2, 1))\n",
    "\n",
    "        # a_{t} = \\text{softmax}(e_t)\n",
    "        s = K.sum(e, axis=-1)\n",
    "        s = K.tile(K.expand_dims(s, axis=-1), K.stack([1, 1, input_len]))\n",
    "        a = e / (s + K.epsilon())\n",
    "\n",
    "        # l_t = \\sum_{t'} a_{t, t'} x_{t'}\n",
    "        v = K.batch_dot(a, inputs)\n",
    "        if self.attention_regularizer_weight > 0.0:\n",
    "            self.add_loss(self._attention_regularizer(a))\n",
    "\n",
    "        if positions is not None:\n",
    "            pos_num = K.shape(positions)[1]\n",
    "            batch_indices = K.tile(K.expand_dims(K.arange(K.shape(inputs)[0]), axis=-1), K.stack([1, pos_num]))\n",
    "            pos_indices = K.stack([batch_indices, positions], axis=-1)\n",
    "            v = tf.gather_nd(v, pos_indices)\n",
    "            a = tf.gather_nd(a, pos_indices)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [v, a]\n",
    "        return v\n",
    "\n",
    "    def _call_additive_emission(self, inputs):\n",
    "        input_shape = K.shape(inputs)\n",
    "        batch_size, input_len = input_shape[0], input_shape[1]\n",
    "\n",
    "        # h_{t, t'} = \\tanh(x_t^T W_t + x_{t'}^T W_x + b_h)\n",
    "        q, k = K.dot(inputs, self.Wt), K.dot(inputs, self.Wx)\n",
    "        q = K.tile(K.expand_dims(q, 2), K.stack([1, 1, input_len, 1]))\n",
    "        k = K.tile(K.expand_dims(k, 1), K.stack([1, input_len, 1, 1]))\n",
    "        if self.use_additive_bias:\n",
    "            h = K.tanh(q + k + self.bh)\n",
    "        else:\n",
    "            h = K.tanh(q + k)\n",
    "\n",
    "        # e_{t, t'} = W_a h_{t, t'} + b_a\n",
    "        if self.use_attention_bias:\n",
    "            e = K.reshape(K.dot(h, self.Wa) + self.ba, (batch_size, input_len, input_len))\n",
    "        else:\n",
    "            e = K.reshape(K.dot(h, self.Wa), (batch_size, input_len, input_len))\n",
    "        return e\n",
    "\n",
    "    def _call_multiplicative_emission(self, inputs):\n",
    "        # e_{t, t'} = x_t^T W_a x_{t'} + b_a\n",
    "        e = K.batch_dot(K.dot(inputs, self.Wa), K.permute_dimensions(inputs, (0, 2, 1)))\n",
    "        if self.use_attention_bias:\n",
    "            e = e + self.ba\n",
    "        return e\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if isinstance(input_shape, list):\n",
    "            input_shape, pos_shape = input_shape\n",
    "            output_shape = (input_shape[0], pos_shape[1], input_shape[2])\n",
    "        else:\n",
    "            output_shape = input_shape\n",
    "        if self.return_attention:\n",
    "            attention_shape = (input_shape[0], output_shape[1], input_shape[1])\n",
    "            return [output_shape, attention_shape]\n",
    "        return output_shape\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if isinstance(inputs, list):\n",
    "            mask = mask[1]\n",
    "        if self.return_attention:\n",
    "            return [mask, None]\n",
    "        return mask\n",
    "\n",
    "    def _attention_regularizer(self, attention):\n",
    "        batch_size = K.cast(K.shape(attention)[0], K.floatx())\n",
    "        input_len = K.shape(attention)[-1]\n",
    "        return self.attention_regularizer_weight * K.sum(K.square(K.batch_dot(\n",
    "            attention,\n",
    "            K.permute_dimensions(attention, (0, 2, 1))) - tf.eye(input_len))) / batch_size\n",
    "\n",
    "    @staticmethod\n",
    "    def get_custom_objects():\n",
    "      return {'SeqSelfAttention': SeqSelfAttention}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model creation using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(seq_len,unique_notes,dropout=0.3,output_emb=100,rnn_unit=128,dense_unit=64):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=(seq_len,))\n",
    "    embedding = tf.keras.layers.Embedding(input_dim=unique_notes+1, output_dim=output_emb,input_length=seq_len)(inputs)\n",
    "    forward_pass = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(rnn_unit,return_sequences=True))(embedding)\n",
    "    forward_pass, att_vector = SeqSelfAttention(return_attention=True,attention_activation='sigmoid',attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,attention_width=50,kernel_regularizer=tf.keras.regularizers.l2(1e-4),bias_regularizer=tf.keras.regularizers.l1(1e-4),attention_regularizer_weight = 1e-4,)(forward_pass)\n",
    "    forward_pass = tf.keras.layers.Dropout(dropout)(forward_pass)\n",
    "    forward_pass = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(rnn_unit,return_sequences=True))(forward_pass)\n",
    "    forward_pass, att_vector2 = SeqSelfAttention(return_attention= True, attention_activation='sigmoid',attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,attention_width=50,kernel_regularizer=tf.keras.regularizers.l2(1e-4),bias_regularizer=tf.keras.regularizers.l1(1e-4),attention_regularizer_weight = 1e-4,)(forward_pass)\n",
    "    forward_pass = tf.keras.layers.Dropout(dropout)(forward_pass)\n",
    "    forward_pass = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(rnn_unit))(forward_pass)\n",
    "    forward_pass = tf.keras.layers.Dropout(dropout)(forward_pass)\n",
    "    forward_pass = tf.keras.layers.LeakyReLU()(forward_pass)\n",
    "    outputs = tf.keras.layers.Dense(unique_notes+1,activation=\"softmax\")(forward_pass)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = inputs, outputs = outputs, name = \"generate_scores_rnn\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(seq_len, unique_notes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generate_scores_rnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 50, 100)           1226000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 50, 256)           176640    \n",
      "_________________________________________________________________\n",
      "seq_self_attention (SeqSelfA [(None, 50, 256), (None,  65537     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 256)           296448    \n",
      "_________________________________________________________________\n",
      "seq_self_attention_1 (SeqSel [(None, 50, 256), (None,  65537     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               296448    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12260)             3150820   \n",
      "=================================================================\n",
      "Total params: 5,277,430\n",
      "Trainable params: 5,277,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model,'this_model.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "optimizer = Nadam()\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,model=model)\n",
    "checkpoint_dir = './checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir,\"ckpt\")\n",
    "loss_fn = sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModel:\n",
    "    \n",
    "    def __init__(self,epochs,note_tokenizer,sampled_midi,frame_per_seconds,batch_nnet_size,batch_song,optimizer,checkpoint,loss_fn,checkpoint_prefix,total_songs,model):\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.note_tokenizer = note_tokenizer\n",
    "        self.sampled_midi = sampled_midi\n",
    "        self.frame_per_second = frame_per_seconds\n",
    "        self.batch_nnet_size = batch_nnet_size\n",
    "        self.batch_song = batch_song\n",
    "        self.optimizer = optimizer\n",
    "        self.checkpoint = checkpoint\n",
    "        self.loss_fn = loss_fn\n",
    "        self.checkpoint_prefix = checkpoint_prefix\n",
    "        self.total_songs = total_songs\n",
    "        self.model = model\n",
    "        \n",
    "    def train(self):\n",
    "            \n",
    "        for epoch in tqdm_notebook(range(self.epochs),desc='epochs'):\n",
    "            #for each epochs, shuffle the list of all the datasets\n",
    "            shuffle(self.sampled_midi)\n",
    "            loss_total = 0\n",
    "            steps = 0\n",
    "            steps_nnet = 0\n",
    "                \n",
    "                \n",
    "            #iterate all song by self.song_size\n",
    "            for i in tqdm_notebook(range(0,self.total_songs,self.batch_song),desc='MUSIC'):\n",
    "                steps += 1\n",
    "                inputs_nnet_large,outputs_nnet_large = generate_batch_song(self.sampled_midi,self.batch_song,start_index=i,fs=self.frame_per_second,seq_len=seq_len,use_tqdm=False)\n",
    "                inputs_nnet_large = np.array(self.note_tokenizer.transform(inputs_nnet_large),dtype=np.int32)\n",
    "                outputs_nnet_large = np.array(self.note_tokenizer.transform(outputs_nnet_large),dtype=np.int32)\n",
    "                    \n",
    "                    \n",
    "                index_shuffled = np.arange(start=0,stop=len(inputs_nnet_large))\n",
    "                np.random.shuffle(index_shuffled)\n",
    "                    \n",
    "                for nnet_steps in tqdm_notebook(range(0,len(index_shuffled),self.batch_nnet_size)):\n",
    "                    steps_nnet +=1\n",
    "                    current_index= index_shuffled[nnet_steps:nnet_steps + self.batch_nnet_size]\n",
    "                    inputs_nnet, outputs_nnet = inputs_nnet_large[current_index], outputs_nnet_large[current_index]\n",
    "                        \n",
    "                    if len(inputs_nnet) // self.batch_nnet_size != 1:\n",
    "                        break\n",
    "                    loss = self.train_step(inputs_nnet,outputs_nnet)\n",
    "                    loss_total += tf.math.reduce_sum(loss)\n",
    "                    if steps_nnet % 20 == 0:\n",
    "                        print(\"epochs {} | Steps {} | total loss {} \".format(epoch+1,steps_nnet,loss_total))\n",
    "                \n",
    "            checkpoint.save(file_prefix = self.checkpoint_prefix)\n",
    "             \n",
    "                \n",
    "    @tf.function\n",
    "    def train_step(self,inputs,targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = self.model(inputs)\n",
    "            loss = self.loss_fn(targets,prediction)\n",
    "        gradients = tape.gradient(loss,self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients,self.model.trainable_variables))\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "print(TOTAL_SONG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846de4b5aba94a918444148a790a43ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epochs', max=4.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:29: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654aa26f73334f709296039aeb1ba89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='MUSIC', max=6.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:39: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321dbb9b664341c0aab1013d393a84ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 1 | Steps 20 | total loss 17390.244140625 \n",
      "epochs 1 | Steps 40 | total loss 33383.12109375 \n",
      "epochs 1 | Steps 60 | total loss 48909.69140625 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:39: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da4cf87e6d144df9ee251ffb39bee64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=110.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 1 | Steps 80 | total loss 63615.34765625 \n",
      "epochs 1 | Steps 100 | total loss 81355.5 \n",
      "epochs 1 | Steps 120 | total loss 98107.515625 \n",
      "epochs 1 | Steps 140 | total loss 114098.4609375 \n",
      "epochs 1 | Steps 160 | total loss 129673.1953125 \n",
      "epochs 1 | Steps 180 | total loss 144740.609375 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4111dc6e703247a8ae9fd20fd9214bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=104.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 1 | Steps 200 | total loss 160268.390625 \n",
      "epochs 1 | Steps 220 | total loss 176685.46875 \n",
      "epochs 1 | Steps 240 | total loss 192326.046875 \n",
      "epochs 1 | Steps 260 | total loss 207604.21875 \n",
      "epochs 1 | Steps 280 | total loss 222559.546875 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf3d78f6b2ed417f95a0fa63b6580814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=38.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 1 | Steps 300 | total loss 237615.90625 \n",
      "epochs 1 | Steps 320 | total loss 252900.59375 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7691fab1ee4091aee776faf45fa4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=84.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 1 | Steps 340 | total loss 267700.09375 \n",
      "epochs 1 | Steps 360 | total loss 282868.15625 \n",
      "epochs 1 | Steps 380 | total loss 296931.90625 \n",
      "epochs 1 | Steps 400 | total loss 310610.09375 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060ed30e2ddc4feab00ded40f5fb5187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 1 | Steps 420 | total loss 324337.90625 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:29: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f53f495aa144a1ba82297be69af9f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='MUSIC', max=6.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f99de604414446acdef95bc3bdc595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 2 | Steps 20 | total loss 14400.8466796875 \n",
      "epochs 2 | Steps 40 | total loss 27500.330078125 \n",
      "epochs 2 | Steps 60 | total loss 40035.17578125 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e4723a2ad74f9aa052a3efc7e6b7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=99.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 2 | Steps 80 | total loss 52069.44921875 \n",
      "epochs 2 | Steps 100 | total loss 66577.9921875 \n",
      "epochs 2 | Steps 120 | total loss 79709.1015625 \n",
      "epochs 2 | Steps 140 | total loss 92432.9609375 \n",
      "epochs 2 | Steps 160 | total loss 104710.34375 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba18cc194ab429381381eae614d4049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=91.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 2 | Steps 180 | total loss 116504.15625 \n",
      "epochs 2 | Steps 200 | total loss 130228.6328125 \n",
      "epochs 2 | Steps 220 | total loss 142881.671875 \n",
      "epochs 2 | Steps 240 | total loss 155186.09375 \n",
      "epochs 2 | Steps 260 | total loss 166766.625 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7088e82a6d034fc89af9027cc4122992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=80.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 2 | Steps 280 | total loss 180112.359375 \n",
      "epochs 2 | Steps 300 | total loss 193961.640625 \n",
      "epochs 2 | Steps 320 | total loss 206947.546875 \n",
      "epochs 2 | Steps 340 | total loss 219102.03125 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18321faf1754435eae871de582a7f98a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=53.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 2 | Steps 360 | total loss 232754.53125 \n",
      "epochs 2 | Steps 380 | total loss 246444.609375 \n",
      "epochs 2 | Steps 400 | total loss 258621.203125 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebec6e4002e2419d977f449414b7c4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=29.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 2 | Steps 420 | total loss 273864.0 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3763a96c0b343a0aa21e72725a18e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='MUSIC', max=6.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7b61c0c25249209abd30ca07ab36f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=62.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 3 | Steps 20 | total loss 11053.7314453125 \n",
      "epochs 3 | Steps 40 | total loss 21195.30078125 \n",
      "epochs 3 | Steps 60 | total loss 30390.2421875 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7271de7c78ca41f398b4b796929a061d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 3 | Steps 80 | total loss 41878.6171875 \n",
      "epochs 3 | Steps 100 | total loss 52598.33203125 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d699ae786c8345e9bb9e85404890feaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 3 | Steps 120 | total loss 65058.28515625 \n",
      "epochs 3 | Steps 140 | total loss 78158.8671875 \n",
      "epochs 3 | Steps 160 | total loss 90611.921875 \n",
      "epochs 3 | Steps 180 | total loss 102797.3984375 \n",
      "epochs 3 | Steps 200 | total loss 114737.0234375 \n",
      "epochs 3 | Steps 220 | total loss 126527.9921875 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86aa81a6e1d14a1f8f399014d8f72018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=87.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 3 | Steps 240 | total loss 137872.609375 \n",
      "epochs 3 | Steps 260 | total loss 148718.421875 \n",
      "epochs 3 | Steps 280 | total loss 158649.390625 \n",
      "epochs 3 | Steps 300 | total loss 168017.515625 \n",
      "epochs 3 | Steps 320 | total loss 177147.0625 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a3ccdcfba741c88e1a506dfa4ff3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=86.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 3 | Steps 340 | total loss 189809.296875 \n",
      "epochs 3 | Steps 360 | total loss 202472.109375 \n",
      "epochs 3 | Steps 380 | total loss 214470.9375 \n",
      "epochs 3 | Steps 400 | total loss 225969.875 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a58439983d5400d96c1a32715cce3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 3 | Steps 420 | total loss 239120.3125 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ea40521be64160b9aa3323bd0364d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='MUSIC', max=6.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6913933e423d4780a89a4527bcfb0a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=91.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 4 | Steps 20 | total loss 12260.4482421875 \n",
      "epochs 4 | Steps 40 | total loss 23639.71875 \n",
      "epochs 4 | Steps 60 | total loss 34952.421875 \n",
      "epochs 4 | Steps 80 | total loss 46171.79296875 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26a62f131394726a26ea1340ef32669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=49.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 4 | Steps 100 | total loss 57462.41796875 \n",
      "epochs 4 | Steps 120 | total loss 68954.0859375 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c6e3645c174339af2e48e0ed31d99d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=90.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 4 | Steps 160 | total loss 91165.0625 \n",
      "epochs 4 | Steps 180 | total loss 102113.3359375 \n",
      "epochs 4 | Steps 200 | total loss 112796.8203125 \n",
      "epochs 4 | Steps 220 | total loss 122973.34375 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf26e7751f444197928ef45aa0fee641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=107.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 4 | Steps 240 | total loss 132931.421875 \n",
      "epochs 4 | Steps 260 | total loss 142239.734375 \n",
      "epochs 4 | Steps 280 | total loss 150976.1875 \n",
      "epochs 4 | Steps 300 | total loss 159361.09375 \n",
      "epochs 4 | Steps 320 | total loss 167632.921875 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e48e7ac37864a278cbb611d4a6a8b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=72.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 4 | Steps 340 | total loss 175861.40625 \n",
      "epochs 4 | Steps 360 | total loss 185211.75 \n",
      "epochs 4 | Steps 380 | total loss 193593.34375 \n",
      "epochs 4 | Steps 400 | total loss 201421.484375 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99ffeba05f4402f841574c605697809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 4 | Steps 420 | total loss 210344.953125 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seq_len = 50\n",
    "EPOCHS = 4\n",
    "BATCH_SONG = 10\n",
    "BATCH_NNET_SIZE = 96\n",
    "TOTAL_SONGS = len(sampled_midi)\n",
    "FRAME_PER_SECOND = 5\n",
    "\n",
    "train_class = TrainModel(EPOCHS, note_tokenizer, sampled_midi,FRAME_PER_SECOND, BATCH_NNET_SIZE, BATCH_SONG, optimizer,checkpoint, loss_fn, checkpoint_prefix,TOTAL_SONGS,model)\n",
    "\n",
    "train_class.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save the model as h5 and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_ep4.h5')\n",
    "pickle.dump(note_tokenizer, open(\"tokenizer.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('model_ep4.h5',custom_objects = SeqSelfAttention.get_custom_objects())\n",
    "note_tokenizer = pickle.load(open(\"tokenizer.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate MIDI Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_random(unique_notes,seq_len=50): # this one generate from 50 notes instead \n",
    "    generate=np.random.randint(0,unique_notes,seq_len).tolist() #low,high,size\n",
    "    return generate\n",
    "\n",
    "def generate_from_one_note(note_tokenizer,new_notes='35'):\n",
    "    generate = [note_tokenizer.notes_to_index['e'] for i in range(49)] # e is the empty note mean no sound\n",
    "    generate += [note_tokenizer.notes_to_index[new_notes]]\n",
    "    return generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5855,\n",
       " 10673,\n",
       " 3029,\n",
       " 2581,\n",
       " 10452,\n",
       " 6321,\n",
       " 11144,\n",
       " 2274,\n",
       " 2900,\n",
       " 7102,\n",
       " 7342,\n",
       " 5697,\n",
       " 1057,\n",
       " 6253,\n",
       " 2277,\n",
       " 2131,\n",
       " 2865,\n",
       " 10912,\n",
       " 602,\n",
       " 11839,\n",
       " 4404,\n",
       " 719,\n",
       " 5820,\n",
       " 8342,\n",
       " 479,\n",
       " 6155,\n",
       " 6616,\n",
       " 5560,\n",
       " 7517,\n",
       " 8584,\n",
       " 5884,\n",
       " 10782,\n",
       " 6705,\n",
       " 78,\n",
       " 5578,\n",
       " 11532,\n",
       " 5871,\n",
       " 6374,\n",
       " 11603,\n",
       " 5275,\n",
       " 7256,\n",
       " 3430,\n",
       " 2953,\n",
       " 6436,\n",
       " 4191,\n",
       " 9991,\n",
       " 5444,\n",
       " 691,\n",
       " 9548,\n",
       " 8661]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_from_random(unique_notes,seq_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_notes(generate,model,unique_notes,max_generated=1000, seq_len=50):\n",
    "    for i in tqdm_notebook(range(max_generated),desc='generating'):\n",
    "        test_input = np.array([generate])[:,i:i+seq_len]\n",
    "        predicted_note = model.predict(test_input)\n",
    "        random_note_pred = choice(unique_notes+1,1,replace=False,p = predicted_note[0])\n",
    "        generate.append(random_note_pred[0])\n",
    "    return generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_midi_file_from_generated(generate, midi_file_name=\"result.mid\",start_index=49,fs=8,max_generated=1000):\n",
    "    note_string= [note_tokenizer.index_to_notes[ind_note] for ind_note in generate]\n",
    "    array_piano_roll = np.zeros((128,max_generated +1),dtype = np.int16)\n",
    "    #because the channel in midi for piano is 128\n",
    "    for index, note in enumerate(note_string[start_index:]):\n",
    "        if note == 'e':\n",
    "            pass\n",
    "        else:\n",
    "            splitted_note = note.split(',')\n",
    "            for j in splitted_note:\n",
    "                array_piano_roll[int(j),index] = 1\n",
    "    \n",
    "    generate_to_midi = piano_roll_to_pretty_midi(array_piano_roll,fs = fs)\n",
    "    print(\"Tempo {}\".format(generate_to_midi.estimate_tempo()))\n",
    "    \n",
    "    for note in generate_to_midi.instruments[0].notes:\n",
    "        note.veloctiy = 1000\n",
    "        \n",
    "    generate_to_midi.write(midi_file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676565dc2d9b4927aa0020ef62dd11b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='generating', max=200.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tempo 209.99999999999994\n"
     ]
    }
   ],
   "source": [
    "max_generate = 200\n",
    "unique_notes = note_tokenizer.unique_word\n",
    "seq_len = 50\n",
    "generate = generate_from_random(unique_notes, seq_len)\n",
    "generate = generate_notes(generate,model,unique_notes,max_generate,seq_len)\n",
    "write_midi_file_from_generated(generate,\"random.mid\",start_index=seq_len-1,fs=7,max_generated= max_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c460c83819345939c087e9effa74ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='generating', max=300.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tempo 240.0\n"
     ]
    }
   ],
   "source": [
    "max_generate = 300\n",
    "unique_notes = note_tokenizer.unique_word\n",
    "seq_len=50\n",
    "generate = generate_from_one_note(note_tokenizer, '72')\n",
    "generate = generate_notes(generate, model, unique_notes, max_generate, seq_len)\n",
    "write_midi_file_from_generated(generate, \"one_note.mid\", start_index=seq_len-1, fs=8, max_generated = max_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
